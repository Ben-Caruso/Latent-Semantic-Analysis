{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Benjamin Caruso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook based on the following paper: http://lsa.colorado.edu/papers/dp1.LSAintro.pdf by researchers in the Department of Psychology at the University of Colorado, Boulder, 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats\n",
    "from scipy import spatial\n",
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) is a method of obtaining the similarity of meaning of words within a large series of texts using pure linear algebra. Every word or set of words is transformed as a point in a very high dimensional vector space. LSA, while related to neural network models, is actually based on singular value decomposition, which breaks the BOW matrix down into three distinct matrices, each containing information that when combined via dot product, builds the original matrix. Many simulation results using LSA compare to human cognition results, implying that LSA does a successful job of capturing the semantics of words and passages within a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text data: titles of NLP and health/exercise research papers - research papers on disjoint topics\n",
    "\n",
    "# Paper titles found from paperswithcode.com and nih website\n",
    "\n",
    "# Exercise and health related titles\n",
    "n1 = 'Damage to Skeletal Muscle from Eccentric Exercise'\n",
    "n2 = 'High-Intensity Interval Training to Maximize Cardiac Benefits of Exercise Training'\n",
    "n3 = 'Exercise Enhances and Protects Brain Function'\n",
    "n4 = 'Skeletal Muscle Hypertrophy After Aerobic Exercise Training'\n",
    "n5 = 'Training-Induced Changes in Neural Function'\n",
    "\n",
    "# NLP related titles\n",
    "t1 = 'Parameter-Efficient Transfer Learning for NLP'\n",
    "t2 = 'Recurrent Attention Network on Memory for Aspect Sentiment Analysis'\n",
    "t3 = 'Bag of Tricks for Efficient Text Classification'\n",
    "t4 = 'SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis'\n",
    "t5 = 'Interactive Attention Networks for Aspect Level Sentiment Classification'\n",
    "t6 = 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\n",
    "\n",
    "# Define entire corpus as set of all titles\n",
    "corpus = [n1, n2, n3, n4, n5, t1, t2, t3, t4, t5, t6]\n",
    "# Used for dataframe index\n",
    "corpus_map = ['n1', 'n2', 'n3', 'n4', 'n5', 't1', 't2', 't3', 't4', 't5', 't6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Skeletal', 'Eccentric', 'Benefits', 'Learning', 'NLP']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of vocabulary over entire corpus, with repeats\n",
    "vocab = [word for sentence in corpus for word in sentence.split(' ')]\n",
    "# Obtain list of unqiue words within vocab\n",
    "unique_words = list(set(vocab))\n",
    "\n",
    "# See first 5 words\n",
    "unique_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skeletal</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Muscle</th>\n",
       "      <th>Function</th>\n",
       "      <th>of</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Attention</th>\n",
       "      <th>to</th>\n",
       "      <th>for</th>\n",
       "      <th>Training</th>\n",
       "      <th>Exercise</th>\n",
       "      <th>Aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>n1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Skeletal  NLP  Analysis  Muscle  Function  of  Classification  Sentiment  \\\n",
       "n1         1    0         0       1         0   0               0          0   \n",
       "n2         0    0         0       0         0   1               0          0   \n",
       "n3         0    0         0       0         1   0               0          0   \n",
       "n4         1    0         0       1         0   0               0          0   \n",
       "n5         0    0         0       0         1   0               0          0   \n",
       "t1         0    1         0       0         0   0               0          0   \n",
       "t2         0    0         1       0         0   0               0          1   \n",
       "t3         0    0         0       0         0   1               1          0   \n",
       "t4         0    0         1       0         0   0               0          2   \n",
       "t5         0    0         0       0         0   0               1          1   \n",
       "t6         0    1         0       0         0   0               0          0   \n",
       "\n",
       "    Attention  to  for  Training  Exercise  Aspect  \n",
       "n1          0   1    0         0         1       0  \n",
       "n2          0   1    0         2         1       0  \n",
       "n3          0   0    0         0         1       0  \n",
       "n4          0   0    0         1         1       0  \n",
       "n5          0   0    0         0         0       0  \n",
       "t1          0   0    1         0         0       0  \n",
       "t2          1   0    1         0         0       1  \n",
       "t3          0   0    1         0         0       0  \n",
       "t4          0   0    1         0         0       0  \n",
       "t5          1   0    1         0         0       1  \n",
       "t6          0   0    1         0         0       0  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map each word to empty list for bag-of-words dictionary \n",
    "bow = {word: [] for word in unique_words}\n",
    "\n",
    "for word in unique_words:\n",
    "    for sentence in corpus:\n",
    "        # For every unique word, we look at each sentence\n",
    "        sentence_words = sentence.split()\n",
    "        \n",
    "        # Obtain number of times word appears in sentence\n",
    "        sum_word = 0\n",
    "        for w in sentence_words:\n",
    "            sum_word += (word == w)\n",
    "        \n",
    "        # Add word count word count vector\n",
    "        bow[word].append(sum_word)\n",
    "        \n",
    "    # Only keep words that appear in at least 2 sentences\n",
    "    if sum(bow[word]) < 2:\n",
    "        del bow[word]   \n",
    "\n",
    "# Convert to dataframe and get matrix\n",
    "bow_df = pd.DataFrame(bow)\n",
    "bow_mat = bow_df.values\n",
    "# Save column names - words that associate with each column\n",
    "columns = list(bow_df.columns)\n",
    "bow_df.index = corpus_map\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we result in a bag of words matrix. This is the matrix that LSA will perform SVD on to determine the most important correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain SVD\n",
    "U, S, V = np.linalg.svd(bow_mat)\n",
    "\n",
    "# Define sigma matrix as same size as A with sigma values on the diagonal\n",
    "sigma = np.zeros(bow_mat.shape)\n",
    "\n",
    "# Check if n or p is greater\n",
    "# Want to use the smaller index to assign the diagonals\n",
    "if bow_mat.shape[1] > bow_mat.shape[0]:\n",
    "    sigma[:bow_mat.shape[0], :bow_mat.shape[0]] = np.diag(S)\n",
    "else:\n",
    "    sigma[:bow_mat.shape[1], :bow_mat.shape[1]] = np.diag(S)\n",
    "\n",
    "# Now recreate matrix using new approximation - zeros are represented as tiny numbers\n",
    "#print(U @ sigma @ V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain first two columns of each matix\n",
    "V2 = V[:,:2]\n",
    "S2 = sigma[:2,:2]\n",
    "U2 = U[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skeletal</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Muscle</th>\n",
       "      <th>Function</th>\n",
       "      <th>of</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Attention</th>\n",
       "      <th>to</th>\n",
       "      <th>for</th>\n",
       "      <th>Training</th>\n",
       "      <th>Exercise</th>\n",
       "      <th>Aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>n1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n4</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Skeletal   NLP  Analysis  Muscle  Function    of  Classification  \\\n",
       "n1      0.15  0.01      0.45    0.74     -0.58 -0.02           -0.51   \n",
       "n2      0.23  0.04      0.66    1.16     -0.88 -0.05           -0.77   \n",
       "n3      0.06  0.00      0.19    0.31     -0.24 -0.01           -0.21   \n",
       "n4      0.17  0.01      0.52    0.85     -0.67 -0.02           -0.59   \n",
       "n5      0.01  0.00      0.02    0.03     -0.02 -0.00           -0.02   \n",
       "t1      0.00  0.20     -0.22    0.23      0.08 -0.16            0.09   \n",
       "t2      0.01  0.60     -0.66    0.65      0.26 -0.47            0.30   \n",
       "t3      0.03  0.26     -0.20    0.41      0.00 -0.20            0.03   \n",
       "t4      0.01  0.60     -0.67    0.65      0.27 -0.47            0.30   \n",
       "t5      0.01  0.57     -0.63    0.64      0.24 -0.45            0.28   \n",
       "t6      0.00  0.20     -0.22    0.23      0.08 -0.16            0.09   \n",
       "\n",
       "    Sentiment  Attention    to   for  Training  Exercise  Aspect  \n",
       "n1      -0.03       0.30  0.06  0.38     -0.01      0.78    0.14  \n",
       "n2      -0.04       0.45  0.11  0.63     -0.05      1.18    0.24  \n",
       "n3      -0.01       0.12  0.02  0.16     -0.01      0.33    0.06  \n",
       "n4      -0.04       0.35  0.07  0.45     -0.01      0.90    0.17  \n",
       "n5      -0.00       0.01  0.00  0.02     -0.00      0.03    0.01  \n",
       "t1       0.11      -0.10  0.18  0.36     -0.26     -0.16    0.15  \n",
       "t2       0.32      -0.30  0.53  1.05     -0.77     -0.49    0.42  \n",
       "t3       0.13      -0.07  0.23  0.51     -0.33     -0.06    0.21  \n",
       "t4       0.32      -0.30  0.53  1.05     -0.77     -0.50    0.43  \n",
       "t5       0.30      -0.28  0.50  1.01     -0.74     -0.46    0.41  \n",
       "t6       0.11      -0.10  0.18  0.36     -0.26     -0.16    0.15  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix using two dimensions\n",
    "corr = (U2 @ S2 @ V2.T)\n",
    "corr_df = pd.DataFrame(corr)\n",
    "\n",
    "# Round values to 2 decimal points\n",
    "corr_df = corr_df.apply(lambda x: round(x, 2))\n",
    "\n",
    "# Assign indices that represent which text refers to which row\n",
    "corr_df.index = corpus_map\n",
    "# Assign column names which match the BOW columns\n",
    "corr_df.columns = columns\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Cosine Similarities and Correlations Among Words and Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r(exercise.training): 0.8 with p-value 0.003\n",
      "Pearson r(exercise.sentiment): -0.87 with p-value 0.0005\n",
      "Value of NLP at document t4: 0.6\n"
     ]
    }
   ],
   "source": [
    "rval, pval = scipy.stats.pearsonr(corr_df['Exercise'], corr_df['Training'])\n",
    "\n",
    "print(f\"Pearson r(exercise.training): {round(rval, 2)} with p-value {round(pval,4)}\")\n",
    "\n",
    "rval, pval = scipy.stats.pearsonr(corr_df['Exercise'], corr_df['Sentiment'])\n",
    "\n",
    "print(f\"Pearson r(exercise.sentiment): {round(rval, 2)} with p-value {round(pval,4)}\")\n",
    "\n",
    "print(f\"Value of NLP at document t4: {corr_df['NLP']['t4']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD on this basic example was able to extract common similarities using only two dimensions of the decomposed matrices. By looking at the r value for exercise with \"training\" and \"sentiment\", we see very different correlations which makes sense since \"training\" appears in many documents with \"exercise\" while \"sentiment\" is associated with a different set of research article titles.\n",
    "\n",
    "Furthermore, and more interestingly, if we look at the NLP vector within the correlation dataframe, we can see that for titles in the NLP set, the word/term NLP is likely to be seen *even in passages that do not include the term NLP*. SVD is able to understand the context that the term NLP is used well enough to extrapolate that NLP likely belongs in a title related to it, still using only two dimensions of interest to make this conclusion. In my toy example, the value of NLP at document t4 is 0.60, implying that NLP would likely be seen 60% of the time that passage t4 appears. t4 refers to the title 'SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis', which does not have the term 'NLP' within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of vectors n1 and n2: 0.9992832039756531\n",
      "Correlation of vectors t9 and t10: 0.9995303171793478\n",
      "Correlation of vectors n1 and t10: -0.0026989886717272427\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlation of vectors n1 and n2: {1 - spatial.distance.cosine(corr_df.values[0], corr_df.values[1])}\")\n",
    "print(f\"Correlation of vectors t9 and t10: {1 - spatial.distance.cosine(corr_df.values[5], corr_df.values[6])}\")\n",
    "print(f\"Correlation of vectors n1 and t10: {1 - spatial.distance.cosine(corr_df.values[0], corr_df.values[10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the passage vectors (vectors of each word's correlation with the vector as a whole) are compared, passages that came from the same subgroup of research titles (NLP and exercise/health) are highly correlated, while passages that are in different subgroups are not correlated whatsoever. In the example shown, vector cosine distance from each of the groups are calculated, and then the cosine distance of vector n1 and t10, in different subgroups, is calculated to be far lower. Each distance was subtracted by 1 to get the similarity measure of the vectors. Although a trivial example, it clearly shows how LSA, through singular value decomposition of a bag-of-words term frequency matrix, has captured some of the similarity between certain passages and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
